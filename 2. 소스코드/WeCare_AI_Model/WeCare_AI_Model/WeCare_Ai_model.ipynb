{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7331e449",
      "metadata": {
        "id": "7331e449"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle # save list as .pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2 # read video file\n",
        "from skimage.transform import resize # resizing images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00dd9ca",
      "metadata": {
        "id": "a00dd9ca"
      },
      "outputs": [],
      "source": [
        "def Save2Npy(file_dir, save_dir):\n",
        "\n",
        "    if not os.path.exists(save_dir): # If there is not save_dir folder, then create new folder at there.\n",
        "        os.makedirs(save_dir)\n",
        "    \n",
        "    file_list=os.listdir(file_dir) # Make list of video file(in file_dir folder)'s name.\n",
        "    \n",
        "    for file in tqdm(file_list):\n",
        "        frames=np.zeros((30, 160, 160, 3), dtype=np.float)\n",
        "        i=0\n",
        "        \n",
        "        vid=cv2.VideoCapture(os.path.join(file_dir, file)) # Create cv2.VideoCapture() Object of each video file.\n",
        "        \n",
        "        if vid.isOpened():\n",
        "            grabbed, frame=vid.read()\n",
        "        else:\n",
        "            grabbed=False\n",
        "            \n",
        "        frm=resize(frame, (160, 160, 3))\n",
        "        frm=np.expand_dims(frm, axis=0)\n",
        "        \n",
        "        if(np.max(frm)>1):\n",
        "            frm=frm/255.0\n",
        "        frames[i][:]=frm\n",
        "        i+=1\n",
        "      \n",
        "        while i<30:\n",
        "            grabbed, frame=vid.read()\n",
        "            if not grabbed:\n",
        "                break\n",
        "            # if type(frame) == type(None):\n",
        "            #   break\n",
        "            frm=resize(frame, (160, 160, 3))\n",
        "            frm=np.expand_dims(frm, axis=0)\n",
        "\n",
        "\n",
        "            if(np.max(frm)>1):\n",
        "                frm=frm/255.0\n",
        "            frames[i][:] = frm\n",
        "            i+=1\n",
        "\n",
        "        video_name=file.split('.')[0]\n",
        "        video_path=os.path.join(file_dir, file)\n",
        "        save_path=os.path.join(save_dir, video_name+'.npy')\n",
        "\n",
        "        np.save(save_path, frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4788277",
      "metadata": {
        "id": "e4788277",
        "outputId": "5e86d9a3-901e-408d-b376-201752ed94ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                              | 0/350 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/2592651303.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  frames=np.zeros((30, 160, 160, 3), dtype=np.float)\n",
            "100%|████████████████████████████████████████████████████████████████████| 350/350 [30:49<00:00,  5.28s/it]\n"
          ]
        }
      ],
      "source": [
        "file_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\violent'\n",
        "save_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_Fight_Numpy'\n",
        "\n",
        "Save2Npy(file_dir=file_dir, save_dir=save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf799a52",
      "metadata": {
        "id": "cf799a52",
        "outputId": "eb3308df-9a40-4c64-8a81-65444d779111"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                              | 0/276 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/2592651303.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  frames=np.zeros((30, 160, 160, 3), dtype=np.float)\n",
            "100%|████████████████████████████████████████████████████████████████████| 276/276 [21:16<00:00,  4.62s/it]\n"
          ]
        }
      ],
      "source": [
        "file_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\non_violent'\n",
        "save_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_NonFight_Numpy'\n",
        "Save2Npy(file_dir=file_dir, save_dir=save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c5ba89",
      "metadata": {
        "id": "a7c5ba89",
        "outputId": "d48dca3f-0569-4716-b813-671f61124a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350\n"
          ]
        }
      ],
      "source": [
        "Fight_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_Fight_Numpy' # Folder that contains Fight(Violence) Video files\n",
        "file_list_npy = os.listdir(Fight_dir) # File name list\n",
        "\n",
        "import gc\n",
        "\n",
        "data_Fight=[]\n",
        "for file in file_list_npy:\n",
        "    file_path=os.path.join(Fight_dir, file)\n",
        "    x=np.load(file_path)\n",
        "    data_Fight.append(x)\n",
        "    gc.collect()\n",
        "\n",
        "print(len(data_Fight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e3d67d",
      "metadata": {
        "id": "f9e3d67d",
        "outputId": "35e97b12-28c2-404b-fd9b-1b300332e28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "276\n"
          ]
        }
      ],
      "source": [
        "NonFight_dir='C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_NonFight_Numpy'\n",
        "file_list_npy=os.listdir(NonFight_dir)\n",
        "\n",
        "import gc\n",
        "\n",
        "data_NonFight=[]\n",
        "for file in file_list_npy:\n",
        "    file_path=os.path.join(NonFight_dir, file)\n",
        "    x=np.load(file_path)\n",
        "    data_NonFight.append(x)\n",
        "    gc.collect()\n",
        "\n",
        "print(len(data_NonFight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1def7a09",
      "metadata": {
        "id": "1def7a09"
      },
      "outputs": [],
      "source": [
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_data_Fight_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(data_Fight, fw, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2743ec7b",
      "metadata": {
        "id": "2743ec7b"
      },
      "outputs": [],
      "source": [
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_data_NonFight_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(data_NonFight, fw, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3174710b",
      "metadata": {
        "id": "3174710b"
      },
      "outputs": [],
      "source": [
        "label_Fight_per_video=np.array([0,1])\n",
        "label_Fight=[label_Fight_per_video]*len(data_Fight) # As amount as count of Violence(Fight) Video\n",
        "\n",
        "label_NonFight_per_video=np.array([1,0])\n",
        "label_NonFight=[label_NonFight_per_video]*len(data_NonFight) # As amount as count of Non-Violence(NonFight) Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b9e91d",
      "metadata": {
        "id": "c6b9e91d",
        "outputId": "636bd803-eeb2-4c7a-a8fd-0b1f85d8dcc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(350, 276)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(label_Fight), len(label_NonFight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59fe4169",
      "metadata": {
        "id": "59fe4169",
        "outputId": "68e95195-2ea7-4f34-a0a1-9a156905cd07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1]), array([1, 0]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_Fight[0], label_NonFight[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a9df71",
      "metadata": {
        "id": "17a9df71"
      },
      "outputs": [],
      "source": [
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_label_Fight_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(label_Fight, fw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5f0218",
      "metadata": {
        "id": "fb5f0218"
      },
      "outputs": [],
      "source": [
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_label_NonFight_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(label_NonFight, fw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20eee71",
      "metadata": {
        "id": "a20eee71"
      },
      "source": [
        "# 02.Making Train set & Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1907160e",
      "metadata": {
        "id": "1907160e"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from random import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cf3dbf",
      "metadata": {
        "id": "30cf3dbf",
        "outputId": "b6a9dadb-e7c7-40ce-9371-0ebe1adcc395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350\n"
          ]
        }
      ],
      "source": [
        "# Fight Video frames Numpy array list : /content/drive/MyDrive/Team2/Violence_detection/1109_projectRVD/datasets/AllVideo_numpy_list_pickle/01_data_Fight_211109.pickle\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_data_Fight_211206.pickle\",\"rb\") as fr:\n",
        "    data_Fight=pickle.load(fr)\n",
        "print(len(data_Fight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49be016c",
      "metadata": {
        "id": "49be016c",
        "outputId": "ab1e3c1f-95e9-4ba3-b1f9-d7bc25dbf8b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350\n"
          ]
        }
      ],
      "source": [
        "# NonFight Video frames Numpy array list : /content/drive/MyDrive/Team2/Violence_detection/1109_projectRVD/datasets/AllVideo_numpy_list_pickle/01_data_NonFight_211109.pickle\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_label_Fight_211206.pickle\",\"rb\") as fr:\n",
        "    label_Fight=pickle.load(fr)\n",
        "print(len(label_Fight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10efe95",
      "metadata": {
        "id": "e10efe95",
        "outputId": "3b1711a8-b6cc-4f19-b131-d939c6b351d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "276\n"
          ]
        }
      ],
      "source": [
        "# Fight label Numpy array list :/content/drive/MyDrive/Team2/Violence_detection/1109_projectRVD/datasets/AllVideo_numpy_list_pickle/01_label_Fight_211109.pickle\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_data_NonFight_211206.pickle\",\"rb\") as fr:\n",
        "    data_NonFight=pickle.load(fr)\n",
        "print(len(data_NonFight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd4c21d",
      "metadata": {
        "id": "efd4c21d",
        "outputId": "006c3402-9bce-46a8-a248-e538eb83913a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "276\n"
          ]
        }
      ],
      "source": [
        "# NonFight label Numpy array list : /content/drive/MyDrive/Team2/Violence_detection/1109_projectRVD/datasets/AllVideo_numpy_list_pickle/01_label_NonFight_211109.pickle\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\01_label_NonFight_211206.pickle\",\"rb\") as fr:\n",
        "    label_NonFight=pickle.load(fr)\n",
        "print(len(label_NonFight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cb77c5",
      "metadata": {
        "id": "36cb77c5",
        "outputId": "1ed050bc-c7c4-42ea-8c33-a18bd3b90061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "626\n",
            "626\n"
          ]
        }
      ],
      "source": [
        "data_total=data_Fight+data_NonFight\n",
        "print(len(data_total))\n",
        "\n",
        "label_total=label_Fight+label_NonFight\n",
        "print(len(label_total))\n",
        "\n",
        "np.random.seed(42)\n",
        "c=list(zip(data_total, label_total)) # zip \n",
        "shuffle(c) # Random Shuffle\n",
        "data_total, label_total=zip(*c) # unpackin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82191479",
      "metadata": {
        "id": "82191479"
      },
      "outputs": [],
      "source": [
        "# Save data\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_total_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(data_total, fw, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save label\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_total_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(label_total, fw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2b4f39",
      "metadata": {
        "id": "0e2b4f39"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_total_211206.pickle\",\"rb\") as fr:\n",
        "    data_total=pickle.load(fr)\n",
        "\n",
        "# load label\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_total_211206.pickle\",\"rb\") as fr:\n",
        "    label_total=pickle.load(fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782e7fa1",
      "metadata": {
        "id": "782e7fa1",
        "outputId": "4e0ef789-c92e-4cf5-849c-303266772de3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 500, 126, 126)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set=int(len(data_total)*0.8)\n",
        "test_set=int(len(data_total)*0.2)\n",
        "\n",
        "data_training=data_total[0:training_set] # Training set data\n",
        "data_test=data_total[training_set:] # Test set data\n",
        "\n",
        "label_training=label_total[0:training_set] # Training set label\n",
        "label_test=label_total[training_set:] # Test set label\n",
        "\n",
        "len(data_training), len(label_training), len(data_test), len(label_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154e474b",
      "metadata": {
        "id": "154e474b",
        "outputId": "f973110d-86d7-443e-9870-21fdb27ffb10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30, 160, 160, 3), (2,))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the shape of elements\n",
        "data_training[0].shape, label_training[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82a27f1",
      "metadata": {
        "id": "c82a27f1",
        "outputId": "c061f27f-dcdc-477e-f8a9-71b39c01fff4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.18823529, 0.63137255, ..., 0.74803922, 0.10637255,\n",
              "        0.        ],\n",
              "       [0.        , 0.18431373, 0.625     , ..., 0.67941176, 0.07254902,\n",
              "        0.        ],\n",
              "       [0.        , 0.18284314, 0.61568627, ..., 0.52009804, 0.04803922,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.17058824, 0.01666667,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.19460784, 0.04656863,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.20147059, 0.0627451 ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_training[0][0, :, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e7339b",
      "metadata": {
        "id": "f2e7339b"
      },
      "outputs": [],
      "source": [
        "# training set, data\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_training_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(data_training, fw, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# training set, label\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_training_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(label_training, fw)\n",
        "\n",
        "# test set, data\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_test_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(data_test, fw, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# test set, label\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_test_211206.pickle\",\"wb\") as fw:\n",
        "    pickle.dump(label_test, fw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6ecaa3",
      "metadata": {
        "id": "7f6ecaa3",
        "outputId": "3c08d4bc-5c9d-4880-c250-44eda05a292b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598a0506",
      "metadata": {
        "id": "598a0506"
      },
      "outputs": [],
      "source": [
        "# Training set : data, label\n",
        "\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_training_211206.pickle\",\"rb\") as fr:\n",
        "    data_training=pickle.load(fr)\n",
        "\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_training_211206.pickle\",\"rb\") as fr:\n",
        "    label_training=pickle.load(fr)\n",
        "\n",
        "# Test set : data, label\n",
        "\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_test_211206.pickle\",\"rb\") as fr:\n",
        "    data_test=pickle.load(fr)\n",
        "\n",
        "with open(\"C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_test_211206.pickle\",\"rb\") as fr:\n",
        "    label_test=pickle.load(fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfda0f8",
      "metadata": {
        "id": "bbfda0f8",
        "outputId": "ac5b79cb-d5a1-4522-d2d0-1e848e3abe03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee72d4e",
      "metadata": {
        "id": "bee72d4e",
        "outputId": "c64e317b-ff1a-46ed-ae3f-684710aa71d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 30, 160, 160, 3), (500, 2))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training set\n",
        "\n",
        "data_training_ar=np.array(data_training, dtype=np.float16) #> (20, 30, 160, 160, 3)\n",
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_training_Numpy_211206.npy', data_training_ar)\n",
        "gc.collect()\n",
        "\n",
        "label_training_ar=np.array(label_training) #> (20, 2)\n",
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_training_Numpy_211206.npy', label_training_ar)\n",
        "gc.collect()\n",
        "\n",
        "data_training_ar.shape, label_training_ar.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7ea7c0",
      "metadata": {
        "id": "4a7ea7c0",
        "outputId": "351a9b8f-6a36-469d-e063-4031564825c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((126, 30, 160, 160, 3), (126, 2))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test set\n",
        "\n",
        "data_test_ar=np.array(data_test, dtype=np.float16) #> (6, 30, 160, 160, 3)\n",
        "\n",
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_test_Numpy_211206.npy', data_test_ar)\n",
        "\n",
        "label_test_ar=np.array(label_test) #> (6, 2)\n",
        "\n",
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_test_Numpy_211206.npy', label_test_ar)\n",
        "\n",
        "data_test_ar.shape, label_test_ar.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec244a3",
      "metadata": {
        "id": "5ec244a3"
      },
      "source": [
        "# 03. Extract features from dataset by using MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc12788",
      "metadata": {
        "id": "1bc12788"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c2597c",
      "metadata": {
        "id": "c9c2597c",
        "outputId": "b5d7b187-707e-44c0-8611-91ec26d5e970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 30, 160, 160, 3)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_training_ar=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_training_Numpy_211206.npy') \n",
        "data_training_ar.shape #> (20, 30, 160, 160, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5e1cb4",
      "metadata": {
        "id": "8a5e1cb4",
        "outputId": "205b0045-25cb-4216-f53d-28d693d15a9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(126, 30, 160, 160, 3)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_test_ar=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_data_test_Numpy_211206.npy') \n",
        "data_test_ar.shape #> (6, 30, 160, 160, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d54af2",
      "metadata": {
        "id": "35d54af2",
        "outputId": "fa7bdca8-937d-482e-9498-fde86a9a831c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15000, 160, 160, 3)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_training_ar=data_training_ar.reshape(data_training_ar.shape[0]*30, 160, 160, 3) \n",
        "data_training_ar.shape #> (600, 160, 160, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef7cc78",
      "metadata": {
        "id": "eef7cc78",
        "outputId": "f74444e1-9179-4c98-b1f7-5ee8b155767d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3780, 160, 160, 3)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_test_ar=data_test_ar.reshape(data_test_ar.shape[0]*30, 160, 160, 3) \n",
        "data_test_ar.shape #> (180, 160, 160, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c009ff7a",
      "metadata": {
        "id": "c009ff7a",
        "outputId": "a40a4f8b-cf72-4831-9fa5-583e015b6bfb"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 1152000000 into shape (450000,160,160,3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2928/3793013255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_training_ar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_training_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_training_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_training_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m                           \u001b[1;31m#> (7200, 160, 160, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_test_ar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_test_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_test_ar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m                               \u001b[1;31m#> (1800, 160, 160, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1152000000 into shape (450000,160,160,3)"
          ]
        }
      ],
      "source": [
        "data_training_ar=data_training_ar.reshape(data_training_ar.shape[0]*30, 160, 160, 3) \n",
        "data_training_ar.shape                           #> (7200, 160, 160, 3)\n",
        "\n",
        "data_test_ar=data_test_ar.reshape(data_test_ar.shape[0]*30, 160, 160, 3) \n",
        "data_test_ar.shape                               #> (1800, 160, 160, 3)\n",
        "\n",
        "X_train=base_model.predict(data_training_ar)\n",
        "print(X_train.shape)                             #> (7200, 160, 160, 3)\n",
        "\n",
        "X_test=base_model.predict(data_test_ar)\n",
        "print(X_test.shape)                              #> (1800, 160, 160, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab9cd5a",
      "metadata": {
        "id": "0ab9cd5a"
      },
      "outputs": [],
      "source": [
        "base_model=keras.applications.mobilenet.MobileNet(input_shape=(160, 160, 3),\n",
        "                                                  include_top=False,\n",
        "                                                  weights='imagenet', classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0cec06",
      "metadata": {
        "id": "cc0cec06",
        "outputId": "b86bcaec-88e9-4683-d27f-03c1493c14c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"mobilenet_1.00_160\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
            "=================================================================\n",
            "Total params: 3,228,864\n",
            "Trainable params: 3,206,976\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary() #> (None, 5, 5, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d627c1",
      "metadata": {
        "id": "07d627c1",
        "outputId": "02098f87-c53d-464f-9a64-69c3c52878d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15000, 5, 5, 1024)\n",
            "(3780, 5, 5, 1024)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train=base_model.predict(data_training_ar)\n",
        "print(X_train.shape)                             #> (600, 5, 5, 1024)\n",
        "\n",
        "X_test=base_model.predict(data_test_ar)\n",
        "print(X_test.shape)                               #> (180, 5, 5, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba66f09",
      "metadata": {
        "id": "0ba66f09",
        "outputId": "958bdf1b-03f0-4fcf-8e12-914cc8188559"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 30, 25600), (126, 30, 25600))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_reshaped = X_train.reshape(int(X_train.shape[0]/30), 30, 5*5*1024) #> (20, 30, 25600) ndarray\n",
        "X_test_reshaped = X_test.reshape(int(X_test.shape[0]/30), 30, 5*5*1024) #> (6, 30, 25600) ndarray\n",
        "\n",
        "X_train_reshaped.shape, X_test_reshaped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2dd318",
      "metadata": {
        "id": "1f2dd318"
      },
      "outputs": [],
      "source": [
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\MobileNet_x_train_reshaped_211206.npy', X_train_reshaped)\n",
        "np.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\MobileNet_x_test_reshaped_211206.npy', X_test_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e860c67",
      "metadata": {
        "id": "1e860c67"
      },
      "source": [
        "# 04. Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a63c56",
      "metadata": {
        "id": "87a63c56"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np \n",
        "import os \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "import time \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e89f659",
      "metadata": {
        "id": "9e89f659"
      },
      "outputs": [],
      "source": [
        "X_train_reshaped=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\MobileNet_x_train_reshaped_211206.npy')\n",
        "\n",
        "X_test_reshaped=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\MobileNet_x_test_reshaped_211206.npy')\n",
        "\n",
        "y_train=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_training_Numpy_211206.npy')\n",
        "\n",
        "y_test=np.load('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\AllVideo_numpy_list_pickle\\\\02_label_test_Numpy_211206.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b10c24",
      "metadata": {
        "id": "a6b10c24",
        "outputId": "da01e334-cf8e-43ec-8cbc-6387041a5491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 30, 25600), (126, 30, 25600))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_reshaped.shape, X_test_reshaped.shape #> (# of video file, # of frame img, 25600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b18c7a",
      "metadata": {
        "id": "95b18c7a",
        "outputId": "38aad344-5e3e-4649-a391-a2ffb0678ace"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 2), (126, 2))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape, y_test.shape #> (# of video file, # of class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d9c856",
      "metadata": {
        "id": "25d9c856",
        "outputId": "96eedbb0-0438-4283-bcf4-82d2f8888655"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1]])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[10:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09787a5",
      "metadata": {
        "id": "e09787a5",
        "outputId": "e881ea05-033b-4f7a-c12f-24f0b804244f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 512)               53479424  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 514       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 54,267,650\n",
            "Trainable params: 54,267,650\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "chunk_size=25600\n",
        "n_chunks=30\n",
        "\n",
        "model=keras.models.Sequential()\n",
        "model.add(keras.layers.LSTM(512, input_shape=(n_chunks, chunk_size))) # (30, 25600)\n",
        "model.add(keras.layers.Dense(1024))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Dense(256))\n",
        "model.add(keras.layers.Activation('sigmoid'))\n",
        "model.add(keras.layers.Dense(2))\n",
        "model.add(keras.layers.Activation('softmax'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2f5ad3",
      "metadata": {
        "id": "3e2f5ad3",
        "outputId": "81972015-ed18-4f6b-fd78-b2005b185483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
          ]
        }
      ],
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0bbeae",
      "metadata": {
        "id": "7e0bbeae"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb=keras.callbacks.ModelCheckpoint('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\211206_MobileNet_checkpoint_epoch50.h5', save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97c6cba",
      "metadata": {
        "id": "e97c6cba",
        "outputId": "5c1c0691-6265-4c48-d099-8eaf9c1493af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 - 15s - loss: 0.2616 - accuracy: 0.5600\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 2/50\n",
            "1/1 - 1s - loss: 0.4398 - accuracy: 0.4400\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 3/50\n",
            "1/1 - 1s - loss: 0.2415 - accuracy: 0.5700\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 4/50\n",
            "1/1 - 1s - loss: 0.2257 - accuracy: 0.5800\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 5/50\n",
            "1/1 - 1s - loss: 0.2172 - accuracy: 0.6320\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 6/50\n",
            "1/1 - 1s - loss: 0.1902 - accuracy: 0.7660\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 7/50\n",
            "1/1 - 1s - loss: 0.1628 - accuracy: 0.8060\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 8/50\n",
            "1/1 - 1s - loss: 0.1348 - accuracy: 0.9160\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 9/50\n",
            "1/1 - 1s - loss: 0.1012 - accuracy: 0.9380\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 10/50\n",
            "1/1 - 1s - loss: 0.0697 - accuracy: 0.9540\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 11/50\n",
            "1/1 - 1s - loss: 0.0446 - accuracy: 0.9720\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 12/50\n",
            "1/1 - 1s - loss: 0.0276 - accuracy: 0.9820\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 13/50\n",
            "1/1 - 1s - loss: 0.0155 - accuracy: 0.9920\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 14/50\n",
            "1/1 - 1s - loss: 0.0095 - accuracy: 0.9940\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 15/50\n",
            "1/1 - 1s - loss: 0.0060 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 16/50\n",
            "1/1 - 2s - loss: 0.0042 - accuracy: 0.9980\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 17/50\n",
            "1/1 - 1s - loss: 0.0032 - accuracy: 0.9980\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 18/50\n",
            "1/1 - 2s - loss: 0.0025 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 19/50\n",
            "1/1 - 1s - loss: 0.0021 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 20/50\n",
            "1/1 - 2s - loss: 0.0018 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 21/50\n",
            "1/1 - 1s - loss: 0.0016 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 22/50\n",
            "1/1 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 23/50\n",
            "1/1 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 24/50\n",
            "1/1 - 1s - loss: 0.0015 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 25/50\n",
            "1/1 - 1s - loss: 0.0012 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 26/50\n",
            "1/1 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 27/50\n",
            "1/1 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 28/50\n",
            "1/1 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 29/50\n",
            "1/1 - 1s - loss: 9.7804e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 30/50\n",
            "1/1 - 1s - loss: 9.1165e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 31/50\n",
            "1/1 - 1s - loss: 8.5106e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 32/50\n",
            "1/1 - 1s - loss: 7.5826e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 33/50\n",
            "1/1 - 1s - loss: 6.4037e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 34/50\n",
            "1/1 - 1s - loss: 6.4613e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 35/50\n",
            "1/1 - 1s - loss: 5.1451e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 36/50\n",
            "1/1 - 1s - loss: 5.5003e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 37/50\n",
            "1/1 - 1s - loss: 4.0155e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 38/50\n",
            "1/1 - 1s - loss: 4.7132e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 39/50\n",
            "1/1 - 1s - loss: 3.8092e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 40/50\n",
            "1/1 - 1s - loss: 3.9524e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 41/50\n",
            "1/1 - 1s - loss: 3.2792e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 42/50\n",
            "1/1 - 1s - loss: 3.3611e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 43/50\n",
            "1/1 - 1s - loss: 3.1918e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 44/50\n",
            "1/1 - 1s - loss: 2.9736e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 45/50\n",
            "1/1 - 1s - loss: 2.8005e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 46/50\n",
            "1/1 - 1s - loss: 2.6158e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 47/50\n",
            "1/1 - 1s - loss: 2.5195e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 48/50\n",
            "1/1 - 1s - loss: 2.3842e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 49/50\n",
            "1/1 - 1s - loss: 2.2175e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 50/50\n",
            "1/1 - 2s - loss: 2.0980e-04 - accuracy: 1.0000\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "epoch=50\n",
        "batchS=500\n",
        "history=model.fit(x=X_train_reshaped[0:2100], y=y_train[0:2100],\n",
        "                  epochs=epoch,\n",
        "                  validation_data=(X_train_reshaped[2100:], y_train[2100:]),\n",
        "                  callbacks=[checkpoint_cb],\n",
        "                  batch_size=batchS, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b8bb657",
      "metadata": {
        "id": "4b8bb657",
        "outputId": "24d0529f-2bca-447b-adbc-adda8cec5481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 63ms/step - loss: 0.2300 - accuracy: 0.7460\n"
          ]
        }
      ],
      "source": [
        "# evaluate LSTM Model by using test set\n",
        "result=model.evaluate(X_test_reshaped, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdc88eb",
      "metadata": {
        "id": "0cdc88eb",
        "outputId": "87feaee2-25b2-4933-bd2f-4f40c188a74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss 0.23004958033561707\n",
            "accuracy 0.7460317611694336\n"
          ]
        }
      ],
      "source": [
        "# accuracy and loss of trained LSTM Model\n",
        "for name, value in zip(model.metrics_names, result):\n",
        "    print(name, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67aab439",
      "metadata": {
        "id": "67aab439",
        "outputId": "d1dbbf05-a89f-4f2d-ac28-8e97ceb804c1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# draw plot of Accuracy and loss\n",
        "\n",
        "plt.plot(history.history['accuracy']) # Draw Accuracy plot\n",
        "# plt.plot(history.history['val_accuracy']) : 없는듯...왜? 학습을 잘못했나?? >> cuda, cudnn version 문제\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['validation'], loc='upper left')\n",
        "plt.savefig('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\211206_MobileNet_model_accuracy_epoch50.jpg') #save .jpg img of Accuracy plot\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss']) # Draw Loss plot\n",
        "# plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\211206_MobileNet_model_loss_epoch50.jpg') #save .jpg img of Loss plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a8fa4f6",
      "metadata": {
        "id": "2a8fa4f6"
      },
      "outputs": [],
      "source": [
        "model.save('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\211206_MobileNet_model_epoch50.h5', include_optimizer=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ad8298",
      "metadata": {
        "id": "03ad8298"
      },
      "source": [
        "# Actual Use : Violence detection for .mp4 video file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4d4098",
      "metadata": {
        "id": "fc4d4098"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import cv2 # openCV 4.5.1\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time \n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize \n",
        "from PIL import Image, ImageFont, ImageDraw # add caption by using custom font\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06659c56",
      "metadata": {
        "id": "06659c56"
      },
      "outputs": [],
      "source": [
        "# base_model : MobileNet\n",
        "base_model=keras.applications.mobilenet.MobileNet(input_shape=(160, 160, 3),\n",
        "                                                  include_top=False,\n",
        "                                                  weights='imagenet', classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ebef5b",
      "metadata": {
        "id": "b5ebef5b"
      },
      "outputs": [],
      "source": [
        "# model : trained LSTM model\n",
        "model=keras.models.load_model('C:\\\\TF2_Object_Detection_API\\\\datasets\\\\211206_MobileNet_model_epoch50.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7884a32",
      "metadata": {
        "id": "e7884a32"
      },
      "outputs": [],
      "source": [
        "# video_reader()\n",
        "# load video file >> Scaling, resizing >> Transform to nparray >> return nparray\n",
        "def video_reader(cv2, filename):\n",
        "    \"\"\"Load 1 video file. Next, read each frame image and resize as (fps, 160, 160, 3) shape and return frame Numpy array.\"\"\"\n",
        "    \n",
        "    frames=np.zeros((30, 160, 160, 3), dtype=np.float) #> (fps, img size, img size, RGB)\n",
        "    \n",
        "    i=0\n",
        "    print(frames.shape)\n",
        "    vid=cv2.VideoCapture(filename) # read frame img from video file.\n",
        "    \n",
        "    if vid.isOpened():\n",
        "        grabbed, frame=vid.read() \n",
        "    else:\n",
        "        grabbed=False\n",
        "    \n",
        "    frm=resize(frame,(160, 160, 3))\n",
        "    frm=np.expand_dims(frm, axis=0)\n",
        "    \n",
        "    if(np.max(frm)>1):\n",
        "        frm=frm/255.0 # Scaling\n",
        "    frames[i][:]=frm\n",
        "    i+=1\n",
        "    print('Reading Video')\n",
        "    \n",
        "    while i<30:\n",
        "        grabbed, frame=vid.read()\n",
        "        frm=resize(frame, (160, 160, 3)) \n",
        "        frm=np.expand_dims(frm, axis=0)\n",
        "        if(np.max(frm)>1):\n",
        "            frm=frm/255.0\n",
        "        frames[i][:]=frm\n",
        "        i+=1\n",
        "        \n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c6026e7",
      "metadata": {
        "id": "2c6026e7"
      },
      "outputs": [],
      "source": [
        "# create_pred_imgarr()\n",
        "# Extract features of each frame img by using base_model(MobileNet)\n",
        "# Reshape features Numpy array to insert LSTM model\n",
        "def create_pred_imgarr(base_model, video_frm_ar):\n",
        "    \"\"\"Insert base_model(MobileNet) and result of video_reader() function.\n",
        "    This function extract features from each frame img by using base_model.\n",
        "    And reshape Numpy array to insert LSTM model : (1, 30, 25600)\"\"\"\n",
        "    video_frm_ar_dim=np.zeros((1, 30, 160, 160, 3), dtype=np.float)\n",
        "    video_frm_ar_dim[0][:][:]=video_frm_ar #> (1, 30, 160, 160, 3)\n",
        "     \n",
        "    # Extract features from each frame img by using base_model(MobileNet)\n",
        "    pred_imgarr=base_model.predict(video_frm_ar)\n",
        "    # Reshape features array : (1, fps, 25600)\n",
        "    pred_imgarr=pred_imgarr.reshape(1, pred_imgarr.shape[0], 5*5*1024)\n",
        "    \n",
        "    return pred_imgarr #> ex : (1, 30, 25600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb8d9a4",
      "metadata": {
        "id": "edb8d9a4"
      },
      "outputs": [],
      "source": [
        "# pred_fight()\n",
        "# Distinguish Violence(Fight) / Non-Violence(NonFight)\n",
        "# Insert reshaped-features-array to trained LSTM model\n",
        "\n",
        "def pred_fight(model, pred_imgarr, acuracy=0.9):\n",
        "    \"\"\"If accuracy>=input value(ex:0.9), return (Violence)'True'. else, return 'False'.\n",
        "    ::model:: trained LSTM model (We already load .h5 file in the above.)\n",
        "    ::pred_imgarr:: (1, 30, 25600) shaped Numpy array. Extracted features.\n",
        "    ::accuracy:: default value is 0.9\"\"\"\n",
        "\n",
        "    pred_test=model.predict(pred_imgarr) #> Violence(Fight) : [0,1]. Non-Violence(NonFight) : [1,0]\n",
        "    \n",
        "    if pred_test[0][1] >= acuracy:\n",
        "        return True, pred_test[0][1] #> True, Probability of Violence\n",
        "    \n",
        "    else:\n",
        "        return False, pred_test[0][1] #> False, Probability of Violence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577b77af",
      "metadata": {
        "id": "577b77af",
        "outputId": "3218b95f-328f-4fdb-fa43-908a49a3a41e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/2150661114.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  frames=np.zeros((30, 160, 160, 3), dtype=np.float) #> (fps, img size, img size, RGB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 160, 160, 3)\n",
            "Reading Video\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/1547568784.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  video_frm_ar_dim=np.zeros((1, 30, 160, 160, 3), dtype=np.float)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(True, 0.9997762)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check above functions doing well\n",
        "\n",
        "# 1. load any video file\n",
        "# video_file='/content/drive/MyDrive/Team2/Violence_detection/1105_jenil123/fight-detection-surv-dataset/fight/fi001.mp4'\n",
        "# (True, 0.9986896)\n",
        "# video_file = '/content/drive/MyDrive/Team2/Violence_detection/1105_jenil123/fight-detection-surv-dataset/noFight/nofi006.mp4'\n",
        "# (False, 0.00013885867)\n",
        "# video_file = '/content/drive/MyDrive/Team2/Violence_detection/1105_Sapir52/dataset_vedio/fi225_xvid.avi'\n",
        "# (False, 0.057196457) >> fight인데 nonfight...\n",
        "video_file = 'C:\\\\Users\\\\user\\\\Downloads\\\\raw\\\\test\\\\violent\\\\v2.mp4'\n",
        "# (False, 0.0019765943)\n",
        "# 2. check function's operation\n",
        "video_frm_ar=video_reader(cv2, video_file) # (30, 160, 160, 3)\n",
        "                                           # Reading Video\n",
        "\n",
        "pred_imgarr=create_pred_imgarr(base_model, video_frm_ar)\n",
        "pred_imgarr.shape  # (1, 30, 25600)\n",
        "\n",
        "preds=pred_fight(model, pred_imgarr, 0.9)\n",
        "preds #> (Violence True or False, Probability of Violence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb0e7f7",
      "metadata": {
        "id": "1cb0e7f7"
      },
      "outputs": [],
      "source": [
        "# define detect_violence()\n",
        "\n",
        "def detect_violence(video):\n",
        "    \"\"\" It contains video_reader(), create_pred_imgarr(), pred_fight() function as all-in-one.\n",
        "    ::video:: video file (.mp4, .avi, ...)\n",
        "    \n",
        "    video_reader() : Read each frame img by using openCV. Resize Numpy array\n",
        "    create_pred_imgarr() : Extract features from frame img array by using base model(MobileNet)\n",
        "    pred_fight() : Decide Violence True or False by using trained LSTM model\"\"\"\n",
        "    \n",
        "    video_frm_ar=video_reader(cv2, video) \n",
        "    pred_imgarr=create_pred_imgarr(base_model, video_frm_ar)\n",
        "    \n",
        "    time1=int(round(time.time()*1000))\n",
        "\n",
        "    f, precent=pred_fight(model, pred_imgarr, acuracy=0.65)\n",
        "    \n",
        "    time2=int(round(time.time()*1000))\n",
        "    \n",
        "    result={'Violence': f, #> True(Violence), False(Non-Violence)\n",
        "            'Violence Estimation': str(precent), # Probability of Violence\n",
        "            'Processing Time' : str(time2-time1)} \n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "123916c6",
      "metadata": {
        "id": "123916c6",
        "outputId": "b3f1ddea-c2d1-4ef6-b09e-249cab75938e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/2150661114.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  frames=np.zeros((30, 160, 160, 3), dtype=np.float) #> (fps, img size, img size, RGB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 160, 160, 3)\n",
            "Reading Video\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2928/1547568784.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  video_frm_ar_dim=np.zeros((1, 30, 160, 160, 3), dtype=np.float)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Violence': True, 'Violence Estimation': '0.9306362', 'Processing Time': '65'}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test function : detect_violence()\n",
        "\n",
        "# video_file='C:\\\\Users\\\\user\\\\Downloads\\WeCare\\\\output05.mp4'\n",
        "'''{'Violence': True, 'Violence Estimation': '0.9997708', 'Processing Time': '94'}'''\n",
        "# video_file='C:\\\\Users\\\\user\\\\Downloads\\\\raw\\\\test\\\\non_violent\\\\u10.mp4'\n",
        "'''{'Violence': False, 'Violence Estimation': '9.745419e-05', 'Processing Time': '78'}'''\n",
        "# video_file='C:\\\\Users\\\\user\\\\Downloads\\\\WeCare\\\\output04.mp4'\n",
        "'''{'Violence': False, 'Violence Estimation': '0.10261435', 'Processing Time': '94'}'''\n",
        "video_file='C:\\\\Users\\\\user\\\\Downloads\\\\WeCare\\\\output09.mp4'\n",
        "'''{'Violence': True, 'Violence Estimation': '0.9306362', 'Processing Time': '65'}'''\n",
        "detect_violence(video_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ff987e",
      "metadata": {
        "id": "d9ff987e",
        "outputId": "95c6dcfe-3a42-4fa0-f3b8-d14e4217cf9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./20211206|123928.mp4\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "video_name = time.strftime('%Y%m%d|%H%M%S')\n",
        "output_path='./' + video_name + '.mp4'\n",
        "print(output_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:WeCare_env] *",
      "language": "python",
      "name": "conda-env-WeCare_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "WeCare_Ai_model.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}